_target_: fusion_bench.method.mingle.c_adamerging_nlp.ContinualLayerWiseAdaMerging

init_values: 0.3
# learning rate
lr: 1e-3
optimizer: adam
batch_size: 8
num_workers: 16
max_steps: 100
seed_buffer_size: 100
# if true, we will use the gradient accumulation across tasks to save memory
use_tta: true
fast_dev_run: false
# shuffle the order of the models
shuffle_order: false
# the random seed to use
seed: 42
# save the merged model on every step
save_on_every_step: true
# evaluate the merged model on every step
evaluate_on_every_step: true



