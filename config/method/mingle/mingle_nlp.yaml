_target_: fusion_bench.method.mingle.mingle_nlp.MINGLE_NLP

# shuffle the order of the models
shuffle_order: true
# the random seed to use
seed: 42
# save the merged model on every step
save_on_every_step: true
# evaluate the merged model on every step
evaluate_on_every_step: true

save_gate_state: false

constraint_gate: true

lora_layer:
  - DenseReluDense.wi
  - SelfAttention
  - EncDecAttention

# lora_layer:
#   - SelfAttention.q
#   - SelfAttention.k
#   - SelfAttention.v
#   - DenseReluDense.wi

batch_reduce: true
# learning rate
lr: 1e-3

optimizer: adam
batch_size: 8
num_workers: 16
max_steps: 50
seed_buffer_size: 100
lora_r: 64
subspace_k: 3
gamma: 1
beta: 0.99
# if true, we will use the gradient accumulation across tasks to save memory
# use_grad_accumulate: true
# fast_dev_run: true